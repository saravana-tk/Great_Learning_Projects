{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R7_InternalLab_Questions_FMNIST_Simple_CNN_CIFAR_DATA_Augment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyfMmMnPJjvn",
        "colab_type": "text"
      },
      "source": [
        "## Train a simple convnet on the Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN2au4CpZT9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import tensorflow\n",
        "try:\n",
        "  # Use the %tensorflow_version magic if in colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjcGOJhcJjvp",
        "colab_type": "text"
      },
      "source": [
        "In this, we will see how to deal with image data and train a convnet for image classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR0Pl2XjJjvq",
        "colab_type": "text"
      },
      "source": [
        "### Load the  `fashion_mnist`  dataset\n",
        "\n",
        "** Use keras.datasets to load the dataset **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr75v_UYJjvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTI42-0qJjvw",
        "colab_type": "text"
      },
      "source": [
        "### Find no.of samples are there in training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2sf67VoJjvx",
        "colab_type": "code",
        "outputId": "4d34014c-8ea3-4592-9901-de94731fdf32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Training dataset shape\n",
        "x_train.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zewyDcBlJjv1",
        "colab_type": "code",
        "outputId": "6673e47e-a6d4-44eb-bcf2-a8d90efdd100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Testing dataset shape\n",
        "x_test.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WytT2eRnJjv4",
        "colab_type": "text"
      },
      "source": [
        "### Find dimensions of an image in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XycQGBSGJjv5",
        "colab_type": "code",
        "outputId": "d1b273e1-7a57-47d6-9767-86c8e0b622e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Find the dimension of an image\n",
        "x_train[0].shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jtdZ7RqJjv8",
        "colab_type": "text"
      },
      "source": [
        "### Convert train and test labels to one hot vectors\n",
        "\n",
        "** check `keras.utils.to_categorical()` **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAD3q5I6Jjv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO5BRBzBJjwD",
        "colab_type": "text"
      },
      "source": [
        "### Normalize both the train and test image data from 0-255 to 0-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fUQpMHxJjwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okwo_SB5JjwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = x_test / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da5-DwgrJjwM",
        "colab_type": "text"
      },
      "source": [
        "### Reshape the data from 28x28 to 28x28x1 to match input dimensions in Conv2D layer in keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPGVQ-JJJjwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This will be done during model building"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFRRTJq8JjwQ",
        "colab_type": "text"
      },
      "source": [
        "### Import the necessary layers from keras to build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWTZYnKSJjwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C18AoS7eJjwU",
        "colab_type": "text"
      },
      "source": [
        "### Build a model \n",
        "\n",
        "** with 2 Conv layers having `32 3*3 filters` in both convolutions with `relu activations` and `flatten` before passing the feature map into 2 fully connected layers (or Dense Layers) having 128 and 10 neurons with `relu` and `softmax` activations respectively. Now, using `categorical_crossentropy` loss with `adam` optimizer train the model with early stopping `patience=5` and no.of `epochs=10`. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DORCLgSwJjwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Clear any existing model in memory\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#Initialize model, reshape & normalize data\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D (28,28) to 3D (28, 28, 1)\n",
        "model.add(tf.keras.layers.Reshape((28,28,1),input_shape=(28,28,)))\n",
        "\n",
        "#Add first convolutional layer\n",
        "model.add(tf.keras.layers.Conv2D(32, #Number of filters \n",
        "                                 kernel_size=(3,3), #Size of the filter\n",
        "                                 activation='relu'))\n",
        "\n",
        "#Add second convolutional layer\n",
        "model.add(tf.keras.layers.Conv2D(32, #Number of filters \n",
        "                                 kernel_size=(3,3), #Size of the filter\n",
        "                                 activation='relu'))\n",
        "\n",
        "#Flatten the output\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "#Dense layer\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\n",
        "#Output layer\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8ymjVlwgBK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGGXUP9MgF-J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "3a64d123-5ca6-4775-c75d-fa6c7a8938c1"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 32)        9248      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               2359424   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 2,370,282\n",
            "Trainable params: 2,370,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uAuIXiAhZDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaT26X17gPEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "1177ccd0-8003-420a-de87-397d7a0aa69e"
      },
      "source": [
        "#Train the model\n",
        "model.fit(x_train,y_train,          \n",
        "          validation_data=(x_test,y_test),\n",
        "          epochs=10,\n",
        "          batch_size=32,\n",
        "          callbacks=[callback])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 13s 219us/sample - loss: 0.3776 - accuracy: 0.8637 - val_loss: 0.2917 - val_accuracy: 0.8967\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 6s 107us/sample - loss: 0.2375 - accuracy: 0.9119 - val_loss: 0.2462 - val_accuracy: 0.9098\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 6s 106us/sample - loss: 0.1721 - accuracy: 0.9351 - val_loss: 0.2370 - val_accuracy: 0.9166\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 6s 106us/sample - loss: 0.1238 - accuracy: 0.9535 - val_loss: 0.2701 - val_accuracy: 0.9114\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 6s 108us/sample - loss: 0.0847 - accuracy: 0.9683 - val_loss: 0.2731 - val_accuracy: 0.9151\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 6s 108us/sample - loss: 0.0599 - accuracy: 0.9780 - val_loss: 0.3193 - val_accuracy: 0.9177\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 6s 107us/sample - loss: 0.0416 - accuracy: 0.9852 - val_loss: 0.3881 - val_accuracy: 0.9145\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 6s 107us/sample - loss: 0.0318 - accuracy: 0.9890 - val_loss: 0.4461 - val_accuracy: 0.9114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f208e48fd30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju69vKdIJjwX",
        "colab_type": "text"
      },
      "source": [
        "### Now, to the above model add `max` pooling layer of `filter size 2x2` and `dropout` layer with `p=0.25` after the 2 conv layers and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2hAP94vJjwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Clear any existing model in memory\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#Initialize model, reshape & normalize data\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D (28,28) to 3D (28, 28, 1)\n",
        "model.add(tf.keras.layers.Reshape((28,28,1),input_shape=(28,28,)))\n",
        "\n",
        "#Add first convolutional layer\n",
        "model.add(tf.keras.layers.Conv2D(32, #Number of filters \n",
        "                                 kernel_size=(3,3), #Size of the filter\n",
        "                                 activation='relu'))\n",
        "\n",
        "#Add second convolutional layer\n",
        "model.add(tf.keras.layers.Conv2D(32, #Number of filters \n",
        "                                 kernel_size=(3,3), #Size of the filter\n",
        "                                 activation='relu'))\n",
        "\n",
        "#Add MaxPooling layer\n",
        "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "#Add another dropout layer\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "#Flatten the output\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "#Dense layer\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\n",
        "#Output layer\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99ZSzdqejEfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp5br3L2ruMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJhdrCxMjIir",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "ff281c7e-d8d2-4e5b-d748-85a92a384391"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               589952    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 600,810\n",
            "Trainable params: 600,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfrGYTjvkkhI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "050a9c57-a169-404a-f927-3b23f6e892e9"
      },
      "source": [
        "#Train the model\n",
        "model.fit(x_train,y_train,          \n",
        "          validation_data=(x_test,y_test),\n",
        "          epochs=10,\n",
        "          batch_size=32,\n",
        "          callbacks=[callback])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3941 - accuracy: 0.8574 - val_loss: 0.2920 - val_accuracy: 0.8928\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2614 - accuracy: 0.9039 - val_loss: 0.2734 - val_accuracy: 0.8984\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.2137 - accuracy: 0.9209 - val_loss: 0.2324 - val_accuracy: 0.9161\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.1840 - accuracy: 0.9324 - val_loss: 0.2313 - val_accuracy: 0.9174\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.1527 - accuracy: 0.9429 - val_loss: 0.2225 - val_accuracy: 0.9248\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.1283 - accuracy: 0.9524 - val_loss: 0.2387 - val_accuracy: 0.9220\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.1123 - accuracy: 0.9568 - val_loss: 0.2385 - val_accuracy: 0.9241\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.0971 - accuracy: 0.9646 - val_loss: 0.2487 - val_accuracy: 0.9236\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.0846 - accuracy: 0.9686 - val_loss: 0.2733 - val_accuracy: 0.9220\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.0747 - accuracy: 0.9719 - val_loss: 0.3017 - val_accuracy: 0.9197\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f208e29f588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGTA3bfEJjwa",
        "colab_type": "text"
      },
      "source": [
        "### Now, to the above model, lets add Data Augmentation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6gX8n5SJjwb",
        "colab_type": "text"
      },
      "source": [
        "### Import the ImageDataGenrator from keras and fit the training images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkUsCc6zp7Kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ImageDataGenerator declaration \n",
        "img_generator= tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20,\n",
        "                                                               width_shift_range=0.2,\n",
        "                                                               height_shift_range=0.2,\n",
        "                                                               horizontal_flip=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5bdwJFcm65j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRsmy4_ooXuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.asarray(x_train).reshape((len(x_train), 28, 28, 1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBIgD9h0qWRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = np.asarray(x_test).reshape((len(x_test), 28, 28, 1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Dp2YZoRxC8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_generator.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl-8dOo7Jjwf",
        "colab_type": "text"
      },
      "source": [
        "#### Showing 5 versions of the first image in training dataset using image datagenerator.flow()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DpI1_McYJjwg",
        "colab_type": "code",
        "outputId": "33563636-0183-4363-9d6b-b606bb1eb1dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "gen = img_generator.flow(x_train[0:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABICAYAAABV5CYrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncJIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkI_i-bZvOGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_generator = img_generator.flow(x_train,y_train, batch_size=64)\n",
        "test_generator = img_generator.flow(x_test,y_test, batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRhVILokrpGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Clear any existing model in memory\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#Initialize model, reshape & normalize data\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D (28,28) to 3D (28, 28, 1)\n",
        "#model.add(tf.keras.layers.Reshape((28,28,1),input_shape=(28,28,)))\n",
        "#model.add(Dense(32, input_shape=(16,)))\n",
        "\n",
        "#Add first convolutional layer\n",
        "model.add(tf.keras.layers.Conv2D(32, #Number of filters \n",
        "                                 kernel_size=(3,3), #Size of the filter\n",
        "                                 activation='relu',input_shape=(28,28,1,)))\n",
        "\n",
        "#Add second convolutional layer\n",
        "model.add(tf.keras.layers.Conv2D(32, #Number of filters \n",
        "                                 kernel_size=(3,3), #Size of the filter\n",
        "                                 activation='relu'))\n",
        "\n",
        "#Add MaxPooling layer\n",
        "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "#Add another dropout layer\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "#Flatten the output\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "#Dense layer\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\n",
        "#Output layer\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNoPiwRXrwWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea8EJylKtUG6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "db09284c-693e-49e4-9df6-ca4ee9a821d5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               589952    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 600,810\n",
            "Trainable params: 600,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmPl5yE8Jjwm",
        "colab_type": "text"
      },
      "source": [
        "### Run the above model using fit_generator()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44ZnDdJYJjwn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "ebc2fcba-d53a-43d6-8c15-4e7da2a40636"
      },
      "source": [
        "model.fit_generator(train_generator,\n",
        "                    epochs=10,\n",
        "                    steps_per_epoch = 60000//64,\n",
        "                    validation_data=test_generator,\n",
        "                    validation_steps = 10000//64)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 937 steps, validate for 156 steps\n",
            "Epoch 1/10\n",
            "937/937 [==============================] - 16s 17ms/step - loss: 0.4157 - accuracy: 0.8446 - val_loss: 0.4333 - val_accuracy: 0.8434\n",
            "Epoch 2/10\n",
            "937/937 [==============================] - 16s 17ms/step - loss: 0.4089 - accuracy: 0.8466 - val_loss: 0.4103 - val_accuracy: 0.8521\n",
            "Epoch 3/10\n",
            "937/937 [==============================] - 15s 17ms/step - loss: 0.4027 - accuracy: 0.8487 - val_loss: 0.4030 - val_accuracy: 0.8482\n",
            "Epoch 4/10\n",
            "937/937 [==============================] - 16s 17ms/step - loss: 0.3958 - accuracy: 0.8528 - val_loss: 0.4003 - val_accuracy: 0.8538\n",
            "Epoch 5/10\n",
            "937/937 [==============================] - 16s 17ms/step - loss: 0.3895 - accuracy: 0.8549 - val_loss: 0.3850 - val_accuracy: 0.8593\n",
            "Epoch 6/10\n",
            "937/937 [==============================] - 16s 17ms/step - loss: 0.3880 - accuracy: 0.8549 - val_loss: 0.3949 - val_accuracy: 0.8570\n",
            "Epoch 7/10\n",
            "937/937 [==============================] - 16s 17ms/step - loss: 0.3795 - accuracy: 0.8583 - val_loss: 0.3861 - val_accuracy: 0.8599\n",
            "Epoch 8/10\n",
            "937/937 [==============================] - 16s 17ms/step - loss: 0.3776 - accuracy: 0.8596 - val_loss: 0.3817 - val_accuracy: 0.8548\n",
            "Epoch 9/10\n",
            "937/937 [==============================] - 16s 17ms/step - loss: 0.3712 - accuracy: 0.8624 - val_loss: 0.3707 - val_accuracy: 0.8626\n",
            "Epoch 10/10\n",
            "937/937 [==============================] - 16s 17ms/step - loss: 0.3705 - accuracy: 0.8634 - val_loss: 0.3714 - val_accuracy: 0.8636\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f207222efd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwQQW5iOJjwq",
        "colab_type": "text"
      },
      "source": [
        "###  Report the final train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1SrtBEPJjwq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a23ef1e0-45a6-4352-a3bb-161efaddb0b3"
      },
      "source": [
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 78us/sample - loss: 0.3326 - accuracy: 0.8833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3326182222247124, 0.8833]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBwVWNQC2qZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model Accuracy is 0.8833"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KXqmUDW2rM1",
        "colab_type": "text"
      },
      "source": [
        "## **DATA AUGMENTATION ON CIFAR10 DATASET**\n",
        "\n",
        "1.   List item\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "2.   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mja6OgQ3L18",
        "colab_type": "text"
      },
      "source": [
        "One of the best ways to improve the performance of a Deep Learning model is to add more data to the training set. Aside from gathering more instances from the wild that are representative of the distinction task, we want to develop a set of methods that enhance the data we already have. There are many ways to augment existing datasets and produce more robust models. In the image domain, these are done to utilize the full power of the convolutional neural network, which is able to capture translational invariance. This translational invariance is what makes image recognition such a difficult task in the first place. You want the dataset to be representative of the many different positions, angles, lightings, and miscellaneous distortions that are of interest to the vision task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HzVTPUM3WZJ",
        "colab_type": "text"
      },
      "source": [
        "### **Import neessary libraries for data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPM558TX4KMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6hicLwP4SqY",
        "colab_type": "text"
      },
      "source": [
        "### **Load CIFAR10 dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ1WzrXd4WNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9Pht1ggHuiT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "751cb21d-793f-404a-a39e-4c83d4de2200"
      },
      "source": [
        "(x_cifar_train, y_cifar_train), (x_cifar_test, y_cifar_test) = cifar10.load_data()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n28ccU6Hp6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN3vYYhK4W0u",
        "colab_type": "text"
      },
      "source": [
        "### **Create a data_gen funtion to genererator with image rotation,shifting image horizontally and vertically with random flip horizontally.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJbekTKi4cmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ImageDataGenerator declaration \n",
        "data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20,\n",
        "                                                               width_shift_range=0.2,\n",
        "                                                               height_shift_range=0.2,\n",
        "                                                               horizontal_flip=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B4Opk6S2Wlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_cifar_train = x_cifar_train / 255\n",
        "x_cifar_test = x_cifar_test / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-SLtUhC4dK2",
        "colab_type": "text"
      },
      "source": [
        "### **Prepare/fit the generator.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSw8Bv2_4hb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen = img_generator.flow(x_cifar_train, batch_size=64)\n",
        "test_gen = img_generator.flow(x_cifar_test, batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYyF-P8O4jQ8",
        "colab_type": "text"
      },
      "source": [
        "### **Generate 5 images for 1 of the image of CIFAR10 train dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXug4z234mwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "56c9498a-3bb1-4d89-8e56-2872d304bd98"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "gen = img_generator.flow(x_cifar_train[5:6], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABICAYAAABV5CYrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}